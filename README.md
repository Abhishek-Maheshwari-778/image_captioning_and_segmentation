# üñºÔ∏è Image Captioning & Panoptic Segmentation

An end-to-end Gradio app that combines **Mask2Former** (panoptic segmentation) with **BLIP-2** (image captioning) to produce a context-aware description of an image and draw labeled boxes for detected objects. Built with PyTorch and ü§ó Transformers.

---

## ‚ú® Features
- **Two-in-one pipeline:** Segments objects and generates a caption that references detected classes.  
- **Upload or URL input:** Choose to upload an image or paste an image URL.  
- **Interactive UI:** Clean Gradio interface with example images and one-click processing.  
- **GPU-aware:** Automatically uses CUDA if available for faster inference.  

---

## üß† Models Used
- **Segmentation:** `facebook/mask2former-swin-large-coco-panoptic` (Mask2Former) for **panoptic segmentation**.  
- **Captioning:** `Salesforce/blip2-opt-2.7b` (BLIP-2 OPT 2.7B) for **context-aware captions**.  

The script auto-downloads models from the Hugging Face Hub on first run.

---

## üèóÔ∏è Project Structure
‚îú‚îÄ‚îÄ image_captioning_and_segmentation.py # Main app (Gradio UI + pipeline)
‚îî‚îÄ‚îÄ README.md # This file


---

## ‚öôÔ∏è Installation
> Python 3.9+ recommended. A GPU with CUDA is ideal for speed, but CPU works too.

```bash
# 1) Create & activate a virtual environment (recommended)
python -m venv .venv
# Windows
.venv\Scripts\activate
# macOS/Linux
source .venv/bin/activate

# 2) Install dependencies
pip install --upgrade pip
pip install gradio transformers Pillow torch torchvision torchaudio

‚ñ∂Ô∏è Usage
python image_captioning_and_segmentation.py
